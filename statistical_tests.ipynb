{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welcome to the universe of STATISTICS**\n",
    "\n",
    "In this script you will notice nothing less but a confidence interval / t-test for the classification results (which are in the csv file). BUT BE CAREFUL WARRIOR; The MCNemar has not been tested as there isn't enough data saved for it yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're in doubt in relation to saving the data as a .csv file, fear no more; as i have used the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# table = np.array([n_list, opt_h, ANN_error_test, opt_lambda, log_test_error, base_line_test_err])\n",
    "\n",
    "# df = pd.DataFrame(np.transpose(table))\n",
    "\n",
    "# df.to_csv('classification_results.csv', index = False, header = [\"test_size\", \"h\", \"ANN\", \"lambda\", \"Log\", \"Base\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "classification_results = pd.read_csv('classification_results.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = classification_results[[\"ANN\", \"Log\", \"Base\"]]\n",
    "names = errors.columns\n",
    "array = np.transpose(errors.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45454545, 0.31818182, 0.27272727, 0.31818182, 0.28571429,\n",
       "       0.38095238, 0.57142857, 0.33333333, 0.42857143, 0.57142857])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence interval for error difference between ANN and Log\n",
      "[ 0.07 , 0.08 ]\n",
      "\n",
      "Confidence interval for error difference between ANN and Base\n",
      "[ 0.33 , 0.34 ]\n",
      "\n",
      "Confidence interval for error difference between Log and Base\n",
      "[ 0.27 , 0.29 ]\n",
      "\n",
      "H_0: The models have the same performance\n",
      "--------------------------------------------------\n",
      "p-value for differnce between ANN and Log\n",
      "p-value:  1.3571134494614427e-07 \n",
      "\n",
      "p-value for differnce between ANN and Base\n",
      "p-value:  1.2294983181543323e-13 \n",
      "\n",
      "p-value for differnce between Log and Base\n",
      "p-value:  4.86068893219175e-10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "## METHOD 11.4.1 (p 216)\n",
    "\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Calculating the difference in error between the models (pair-wise)\n",
    "\n",
    "# index for which models to compare\n",
    "index_for_models = [(0,1), (0,2), (1,2)]\n",
    "\n",
    "#* confidence intervals\n",
    "for (i,j) in index_for_models:\n",
    "    error_for_model_1 = array[i,:]\n",
    "    error_for_model_2 = array[j,:]\n",
    "    # obtaining an array of the difference in generalized error for the models\n",
    "    #TODO: abs or not?\n",
    "    r_list = abs(error_for_model_1 - error_for_model_2)\n",
    "    \n",
    "    # confidence interval:\n",
    "    alpha = 0.05 # 95%-confidence interval\n",
    "    J = 10 # J is the amount of folds\n",
    "    K = 10 # What is K?\n",
    "    sigma_hat = np.var(r_list) # Estimated variance of the errors\n",
    "    r_hat = np.mean(r_list)\n",
    "    c1 = stats.t.ppf(alpha/2, df = J - 1, loc =  r_hat, scale = sigma_hat*(1/J + 1/(K-1)))\n",
    "    c2 = stats.t.ppf(1-alpha/2, df = J - 1, loc = 1/J * np.sum(r_list), scale = sigma_hat*(1/J + 1/(K-1)))\n",
    "    print(\"Confidence interval for error difference between {} and {}\".format(names[i], names[j]))\n",
    "    print(\"[ {:.2f} , {:.2f} ]\\n\".format(c1,c2))\n",
    "\n",
    "\n",
    "#* p-values\n",
    "print(\"H_0: The models have the same performance\")\n",
    "print(\"-\"*50)\n",
    "for (i,j) in index_for_models:\n",
    "    error_for_model_1 = array[i,:]\n",
    "    error_for_model_2 = array[j,:]\n",
    "    r_list = abs(error_for_model_1 - error_for_model_2)\n",
    "    r_hat = np.mean(r_list)\n",
    "    sigma_hat = np.var(r_list)\n",
    "    J = 10\n",
    "    K = 10\n",
    "    P = 1/K\n",
    "    t_hat = r_hat/(sigma_hat * np.sqrt(1/J + P/(1-P)))\n",
    "    p = 2 * stats.t.cdf(- abs(t_hat), df = J - 1, loc = 0, scale = 1)\n",
    "    print(\"p-value for differnce between {} and {}\".format(names[i], names[j]))\n",
    "    print(\"p-value: \", p, \"\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOESNT WORK YEt\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "(y_test) = \"abe\"\n",
    "(y_test_est_ann) = \"abe\"\n",
    "(y_test_est_log) = \"abe\"\n",
    "base = (y_test == big_class)\n",
    "\n",
    "index_for_models = [(0,1), (0,2), (1,2)]\n",
    "\n",
    "\n",
    "# McNemar for the first 3 folds\n",
    "for i in range(3):\n",
    "    for (i,j) in index_for_models:\n",
    "        #comparing the two models; calculatting the different counts\n",
    "        array_1 = (model_1 == y_test)\n",
    "        array_2 = (model_2 == y_test)\n",
    "\n",
    "        n = len(y_test)\n",
    "\n",
    "        n11 = np.dot(array_1, array_2)\n",
    "        n12 = np.dot(array_1, (1 - array_2))\n",
    "        n21 = np.dot((1 - array_1, array_2))\n",
    "        n22 = np.dot((1 - array_1), (1-array_2))\n",
    "\n",
    "        E_theta = (n12 - n21)/n\n",
    "\n",
    "        Q = (n**2 * (n+1) * (E_theta + 1)*(1 - E_theta))/(n * (n12 + n21) - (n12 - n21)**2)\n",
    "\n",
    "        f = (E_theta+1)/2 * (Q - 1)\n",
    "        g = (1 - E_theta)/2 * (Q - 1)\n",
    "\n",
    "        alpha = 0.05\n",
    "\n",
    "        L = 2 * stats.beta.ppf(alpha/2, a = f, b = g) - 1\n",
    "        U = 2 * stats.beta.ppf(1 - alpha/2, a = f, b = g) - 1\n",
    "        print(\"Confidence interval (mcnemar) between {} and {}\".format(names[i], names[j]))\n",
    "        print(\"[ {:.2f} , {:.2f} ]\\n\".format(L,U))\n",
    "    \n",
    "    print(\"H_0: The models have the same performance\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    for (i,j) in index_for_models:\n",
    "        array_1 = (model_1 == y_test)\n",
    "        array_2 = (model_2 == y_test)\n",
    "\n",
    "        n = len(y_test)\n",
    "\n",
    "        n12 = np.dot(array_1, (1 - array_2))\n",
    "        n21 = np.dot((1 - array_1, array_2))\n",
    "\n",
    "        m = min(n21, n21)\n",
    "        p = 2 * stats.binom.cdf(m, n = n12 + n21, p = 1/2)\n",
    "        print(\"p-value : {}\".format(p))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('AndSem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d62badb4c3b216db4db7804bb7696037312a1bbdb2c7465cfb03cc785a825525"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
