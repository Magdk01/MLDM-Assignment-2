{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASSIFICATION**\n",
    "\n",
    "Welcome dear reader. In this jupyter-notebook, you will meet the classification of nothing less but glass. Have you ever wondered about how a computer would classify your bedroom window? Wonder no more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from toolbox_02450 import rocplot, confmatplot\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "font_size = 15\n",
    "plt.rcParams.update({'font.size': font_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "filename = 'glass.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# defining x and y for the linear regression\n",
    "#       all attributes except RI\n",
    "X = np.array(df.iloc[:,1:10])\n",
    "#       number of attributes\n",
    "M = 9\n",
    "#       type\n",
    "y = np.array(df.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the format of C needs to be this, so it is usable with torch, see before and after:\n",
    "print(y)\n",
    "y[y>3] = y[y>3] - 1\n",
    "y = y - 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 1/10. inner_loop: 10/10.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.6041638\t4.3099346e-05\n",
      "\t\t2000\t1.5512775\t2.5973233e-05\n",
      "\t\t3000\t1.5186186\t1.5620959e-05\n",
      "\t\t4000\t1.4994612\t1.0414577e-05\n",
      "\t\t5000\t1.4876357\t6.250365e-06\n",
      "\t\t6000\t1.4801749\t4.0268487e-06\n",
      "\t\t7000\t1.4748989\t3.0713556e-06\n",
      "\t\t8000\t1.4707879\t2.3504826e-06\n",
      "\t\t9000\t1.4672636\t2.3561283e-06\n",
      "\t\t10000\t1.4624377\t2.6084442e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t10000\t1.4624377\t2.6084442e-06\n",
      "\t\t1000\t1.6727899\t4.254265e-05\n",
      "\t\t2000\t1.5942508\t3.364739e-05\n",
      "\t\t3000\t1.5545429\t2.3081491e-05\n",
      "\t\t4000\t1.5235143\t1.572725e-05\n",
      "\t\t5000\t1.5082074\t7.824937e-06\n",
      "\t\t6000\t1.49869\t6.204263e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t6715\t1.4935346\t7.1835154e-07\n",
      "\t\t1000\t1.599934\t4.410731e-05\n",
      "\t\t2000\t1.5615548\t1.3588357e-05\n",
      "\t\t3000\t1.5418086\t1.3066542e-05\n",
      "\t\t4000\t1.5131378\t2.2058708e-05\n",
      "\t\t5000\t1.4895768\t1.6885826e-05\n",
      "\t\t6000\t1.4617945\t1.0030545e-05\n",
      "\t\t7000\t1.4516252\t5.4199736e-06\n",
      "\t\t8000\t1.4445194\t4.6213913e-06\n",
      "\t\t9000\t1.4387635\t3.977042e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "1it [01:18, 78.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t10000\t1.4307209\t6.832286e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t10000\t1.4307209\t6.832286e-06\n",
      "\t\t1000\t1.5772481\t3.3934517e-0510/10.\n",
      "\t\t2000\t1.5484796\t9.007133e-06\n",
      "\t\t3000\t1.5393186\t5.4984157e-06\n",
      "\t\t4000\t1.5317684\t3.7355676e-06\n",
      "\t\t5000\t1.5233885\t6.494933e-06\n",
      "\t\t6000\t1.5138925\t4.8033344e-06\n",
      "\t\t7000\t1.5079952\t3.082999e-06\n",
      "\t\t8000\t1.5039372\t2.5364673e-06\n",
      "\t\t9000\t1.5005602\t2.2244044e-06\n",
      "\t\t10000\t1.4975656\t1.6716402e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t10000\t1.4975656\t1.6716402e-06\n",
      "\t\t1000\t1.592652\t5.373909e-05\n",
      "\t\t2000\t1.5394487\t2.8573197e-05\n",
      "\t\t3000\t1.5092288\t8.767468e-06\n",
      "\t\t4000\t1.4988106\t5.2493433e-06\n",
      "\t\t5000\t1.4914703\t5.1952516e-06\n",
      "\t\t6000\t1.4848961\t3.2915198e-06\n",
      "\t\t7000\t1.4808583\t2.1734988e-06\n",
      "\t\t8000\t1.4777822\t1.936021e-06\n",
      "\t\t9000\t1.4752358\t1.4545228e-06\n",
      "\t\t10000\t1.4732205\t1.2137607e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t10000\t1.4732205\t1.2137607e-06\n",
      "\t\t1000\t1.6597729\t0.0001445581\n",
      "\t\t2000\t1.537581\t3.3414493e-05\n",
      "\t\t3000\t1.5040683\t1.529654e-05\n",
      "\t\t4000\t1.4866844\t8.980602e-06\n",
      "\t\t5000\t1.472609\t8.74264e-06\n",
      "\t\t6000\t1.4599401\t7.430418e-06\n",
      "\t\t7000\t1.450051\t5.9191148e-06\n",
      "\t\t8000\t1.4424278\t3.4710736e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "2it [04:20, 139.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFinal Loss:\n",
      "\t\t8279\t1.4406371\t6.619805e-07\n",
      "outer_loop: 3/10. inner_loop: 1/10.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.586103\t4.599495e-05: 10/10.\n",
      "\t\t2000\t1.5442084\t1.3895387e-05\n",
      "\t\t3000\t1.4775347\t7.494723e-05\n",
      "\t\t4000\t1.4277679\t1.3943215e-05\n",
      "\t\t5000\t1.4151522\t5.89661e-06\n",
      "\t\t6000\t1.4088249\t3.3846331e-06\n",
      "\t\t7000\t1.4038152\t6.8783174e-06\n",
      "\t\t8000\t1.3993559\t1.7037709e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t8810\t1.3976083\t9.382464e-07\n",
      "\t\t1000\t1.6166276\t4.7780904e-05\n",
      "\t\t2000\t1.561494\t1.9543451e-05\n",
      "\t\t3000\t1.5367652\t1.9470084e-05\n",
      "\t\t4000\t1.5196491\t7.2169128e-06\n",
      "\t\t5000\t1.5106324\t4.8137003e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t5314\t1.5084535\t0.0\n",
      "\t\t1000\t1.5828704\t6.242983e-05\n",
      "\t\t2000\t1.5162019\t2.7517512e-05\n",
      "\t\t3000\t1.4837631\t1.7835724e-05\n",
      "\t\t4000\t1.456158\t1.3262055e-05\n",
      "\t\t5000\t1.4421248\t6.943581e-06\n",
      "\t\t6000\t1.4331365\t4.9908167e-06\n",
      "\t\t7000\t1.4269009\t3.5923877e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "3it [07:10, 153.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFinal Loss:\n",
      "\t\t7943\t1.4224936\t9.218326e-07\n",
      "\t\t1000\t1.610741\t2.235021e-05: 10/10.\n",
      "\t\t2000\t1.5584708\t2.1340587e-05\n",
      "\t\t3000\t1.5330882\t1.3296376e-05\n",
      "\t\t4000\t1.5178218\t7.775371e-06\n",
      "\t\t5000\t1.5079337\t3.3993417e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t5216\t1.5062364\t7.91438e-08\n",
      "\t\t1000\t1.5776497\t3.709923e-05\n",
      "\t\t2000\t1.5450529\t1.1496034e-05\n",
      "\t\t3000\t1.4871664\t2.5008881e-05\n",
      "\t\t4000\t1.4662136\t8.862077e-06\n",
      "\t\t5000\t1.4530739\t1.222372e-05\n",
      "\t\t6000\t1.4294456\t6.5048043e-06\n",
      "\t\t7000\t1.421969\t4.275513e-06\n",
      "\t\t8000\t1.416834\t3.0289514e-06\n",
      "\t\t9000\t1.4129423\t2.3623415e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t9839\t1.4103891\t5.071334e-07\n",
      "\t\t1000\t1.5782975\t4.9620947e-05\n",
      "\t\t2000\t1.524351\t2.2209246e-05\n",
      "\t\t3000\t1.5004911\t1.2155219e-05\n",
      "\t\t4000\t1.4864444\t7.859305e-06\n",
      "\t\t5000\t1.475975\t6.7035717e-06\n",
      "\t\t6000\t1.4655374\t7.64606e-06\n",
      "\t\t7000\t1.4561605\t6.549196e-06\n",
      "\t\t8000\t1.4490641\t3.455177e-06\n",
      "\t\t9000\t1.4435302\t4.129072e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "4it [09:52, 156.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t10000\t1.4391187\t2.6507105e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t10000\t1.4391187\t2.6507105e-06\n",
      "\t\t1000\t1.5530065\t5.0889506e-0510/10.\n",
      "\t\t2000\t1.5007286\t2.2638256e-05\n",
      "\t\t3000\t1.4742322\t1.4959237e-05\n",
      "\t\t4000\t1.4568814\t9.737078e-06\n",
      "\t\t5000\t1.4437734\t8.504418e-06\n",
      "\t\t6000\t1.4331906\t6.4046308e-06\n",
      "\t\t7000\t1.4248656\t5.9400754e-06\n",
      "\t\t8000\t1.4181024\t4.2031093e-06\n",
      "\t\t9000\t1.41251\t3.2914081e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t9909\t1.4082422\t7.6185955e-07\n",
      "\t\t1000\t1.5940372\t7.2087074e-05\n",
      "\t\t2000\t1.4709917\t4.2625266e-05\n",
      "\t\t3000\t1.4325696\t1.6059968e-05\n",
      "\t\t4000\t1.416205\t8.249098e-06\n",
      "\t\t5000\t1.4074715\t4.488946e-06\n",
      "\t\t6000\t1.402384\t2.7201443e-06\n",
      "\t\t7000\t1.3993415\t1.4482205e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t7569\t1.3981249\t8.5263616e-07\n",
      "\t\t1000\t1.5938003\t5.818761e-05\n",
      "\t\t2000\t1.5449141\t1.728408e-05\n",
      "\t\t3000\t1.5280195\t6.7092924e-06\n",
      "\t\t4000\t1.5163151\t4.79566e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "5it [12:11, 150.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFinal Loss:\n",
      "\t\t4921\t1.5107968\t2.367148e-07\n",
      "\t\t1000\t1.5214615\t6.44794e-05: 10/10.\n",
      "\t\t2000\t1.4532852\t2.633012e-05\n",
      "\t\t3000\t1.422855\t1.2902224e-05\n",
      "\t\t4000\t1.410104\t6.171335e-06\n",
      "\t\t5000\t1.4031005\t4.0781274e-06\n",
      "\t\t6000\t1.3984691\t2.6425168e-06\n",
      "\t\t7000\t1.3952782\t2.1359367e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t7616\t1.3938221\t9.407951e-07\n",
      "\t\t1000\t1.574144\t6.709192e-05\n",
      "\t\t2000\t1.5091426\t2.5908508e-05\n",
      "\t\t3000\t1.4812526\t1.3922622e-05\n",
      "\t\t4000\t1.4647748\t9.4404595e-06\n",
      "\t\t5000\t1.4541467\t6.2303543e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t5249\t1.4520112\t4.925963e-07\n",
      "\t\t1000\t1.5345286\t0.00011231936\n",
      "\t\t2000\t1.4492754\t3.3394175e-05\n",
      "\t\t3000\t1.4184257\t3.7734055e-05\n",
      "\t\t4000\t1.4002713\t7.661911e-06\n",
      "\t\t5000\t1.3916065\t5.653731e-06\n",
      "\t\t6000\t1.3843981\t4.563762e-06\n",
      "\t\t7000\t1.3787842\t3.4583768e-06\n",
      "\t\t8000\t1.3743689\t2.8623285e-06\n",
      "\t\t9000\t1.3707726\t2.4350152e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "6it [14:23, 144.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t10000\t1.3679247\t1.8300645e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t10000\t1.3679247\t1.8300645e-06\n",
      "\t\t1000\t1.6147382\t3.9938146e-0510/10.\n",
      "\t\t2000\t1.5532521\t1.7575027e-05\n",
      "\t\t3000\t1.536791\t5.972875e-06\n",
      "\t\t4000\t1.5320596\t2.4899082e-06\n",
      "\t\t5000\t1.5291866\t1.792985e-06\n",
      "\t\t6000\t1.5253947\t2.4226379e-06\n",
      "\t\t7000\t1.5225165\t1.3310563e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t7224\t1.522076\t9.3984147e-07\n",
      "\t\t1000\t1.6001778\t0.000102498096\n",
      "\t\t2000\t1.5010948\t3.224141e-05\n",
      "\t\t3000\t1.4706328\t1.2888352e-05\n",
      "\t\t4000\t1.4575258\t5.3980316e-06\n",
      "\t\t5000\t1.4503969\t4.3560935e-06\n",
      "\t\t6000\t1.4414909\t2.8117447e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t6653\t1.4392563\t7.454425e-07\n",
      "\t\t1000\t1.5901083\t3.553418e-05\n",
      "\t\t2000\t1.5501586\t1.853288e-05\n",
      "\t\t3000\t1.528625\t9.82597e-06\n",
      "\t\t4000\t1.5137862\t1.2520949e-05\n",
      "\t\t5000\t1.5008478\t5.4804996e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "7it [16:59, 148.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t6000\t1.4942822\t3.7495029e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t6353\t1.492562\t7.9868835e-07\n",
      "outer_loop: 8/10. inner_loop: 3/10.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 8/10. inner_loop: 4/10.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 8/10. inner_loop: 8/10.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 8/10. inner_loop: 9/10.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 8/10. inner_loop: 10/10.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.623943\t3.4133234e-05\n",
      "\t\t2000\t1.5647762\t3.5957066e-05\n",
      "\t\t3000\t1.5219963\t2.0050618e-05\n",
      "\t\t4000\t1.4981273\t1.1299125e-05\n",
      "\t\t5000\t1.4842985\t7.790354e-06\n",
      "\t\t6000\t1.4745184\t6.063432e-06\n",
      "\t\t7000\t1.4666188\t5.1207217e-06\n",
      "\t\t8000\t1.4596012\t4.5736397e-06\n",
      "\t\t9000\t1.4533753\t3.7730147e-06\n",
      "\t\t10000\t1.4481161\t3.12816e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t10000\t1.4481161\t3.12816e-06\n",
      "\t\t1000\t1.5537531\t5.638855e-05\n",
      "\t\t2000\t1.4998606\t2.257188e-05\n",
      "\t\t3000\t1.4732753\t1.4402568e-05\n",
      "\t\t4000\t1.4546402\t1.0735473e-05\n",
      "\t\t5000\t1.4427233\t7.684342e-06\n",
      "\t\t6000\t1.4338078\t5.98617e-06\n",
      "\t\t7000\t1.4266276\t4.094433e-06\n",
      "\t\t8000\t1.4205457\t3.3567069e-06\n",
      "\t\t9000\t1.4152125\t3.4535901e-06\n",
      "\t\t10000\t1.4103985\t3.0427723e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t10000\t1.4103985\t3.0427723e-06\n",
      "\t\t1000\t1.6059867\t9.760038e-05\n",
      "\t\t2000\t1.5394446\t3.08188e-05\n",
      "\t\t3000\t1.5047932\t1.7348817e-05\n",
      "\t\t4000\t1.4828197\t1.342556e-05\n",
      "\t\t5000\t1.4686079\t7.792415e-06\n",
      "\t\t6000\t1.4587326\t5.5570067e-06\n",
      "\t\t7000\t1.4500363\t5.4259126e-06\n",
      "\t\t8000\t1.4431146\t5.0389135e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "8it [19:49, 155.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFinal Loss:\n",
      "\t\t8552\t1.4398788\t7.451202e-07\n",
      "\t\t1000\t1.6049416\t9.87036e-05: 10/10.\n",
      "\t\t2000\t1.5230931\t3.0210495e-05\n",
      "\t\t3000\t1.4896922\t1.6244358e-05\n",
      "\t\t4000\t1.4710997\t1.0210197e-05\n",
      "\t\tFinal Loss:\n",
      "\t\t4538\t1.4639736\t8.957151e-07\n",
      "\t\t1000\t1.5946923\t4.6868427e-05\n",
      "\t\t2000\t1.5474498\t1.8796422e-05\n",
      "\t\t3000\t1.5299034\t6.3114385e-06\n",
      "\t\t4000\t1.5231841\t3.130519e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t4660\t1.5203425\t4.7045677e-07\n",
      "\t\t1000\t1.5798529\t0.00011943249\n",
      "\t\t2000\t1.4656572\t4.099118e-05\n",
      "\t\t3000\t1.427107\t1.5369676e-05\n",
      "\t\t4000\t1.4108068\t8.280662e-06\n",
      "\t\t5000\t1.4017015\t5.442923e-06\n",
      "\t\t6000\t1.3949963\t3.8454564e-06\n",
      "\t\t7000\t1.390671\t2.4858941e-06\n",
      "\t\t8000\t1.3877113\t1.7180676e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "9it [22:13, 151.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t9000\t1.3855221\t1.3766262e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t9427\t1.3847538\t9.46956e-07\n",
      "\t\t1000\t1.5494926\t9.16974e-05p: 10/10.\n",
      "\t\t2000\t1.4752246\t3.547326e-05\n",
      "\t\t3000\t1.4469836\t1.202802e-05\n",
      "\t\t4000\t1.4335507\t7.1514214e-06\n",
      "\t\t5000\t1.4208446\t5.5373894e-06\n",
      "\t\t6000\t1.4144167\t3.7083764e-06\n",
      "\t\t7000\t1.4100341\t2.7899296e-06\n",
      "\t\t8000\t1.4066143\t2.118723e-06\n",
      "\t\t9000\t1.4040706\t1.6980497e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t9456\t1.4031864\t9.3451655e-07\n",
      "\t\t1000\t1.5658559\t5.3288466e-05\n",
      "\t\t2000\t1.5091842\t2.5196927e-05\n",
      "\t\t3000\t1.4796327\t1.46629445e-05\n",
      "\t\t4000\t1.462137\t8.560667e-06\n",
      "\t\t5000\t1.4497613\t8.633744e-06\n",
      "\t\t6000\t1.4399672\t5.8777796e-06\n",
      "\t\t7000\t1.4316818\t5.7452667e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t7993\t1.4243736\t8.369242e-08\n",
      "\t\t1000\t1.6033188\t3.6877023e-05\n",
      "\t\t2000\t1.5565553\t2.3128232e-05\n",
      "\t\t3000\t1.5301028\t1.20758e-05\n",
      "\t\t4000\t1.5126386\t8.274859e-06\n",
      "\t\t5000\t1.5023817\t5.4749044e-06\n",
      "\t\t6000\t1.4955722\t3.8259764e-06\n",
      "\t\t7000\t1.4906843\t3.0388321e-06\n",
      "\t\t8000\t1.4867767\t2.485564e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/4m0jgf1s5rggf6q996sfswf00000gn/T/ipykernel_47516/1964271695.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
      "10it [24:31, 147.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t9000\t1.4831392\t2.4112842e-06\n",
      "\t\tFinal Loss:\n",
      "\t\t9238\t1.4723316\t5.6676464e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# K = K_outer = K_inner\n",
    "K = 10\n",
    "CV_outer = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "\n",
    "#* INIT FOR LOGISTIC REGRESSION\n",
    "# init for lambda-inner-loop\n",
    "lambda_interval = np.logspace(-8, 2, 50)\n",
    "train_error_rate = np.empty((K,len(lambda_interval)))\n",
    "test_error_rate = np.empty((K,len(lambda_interval)))\n",
    "# init for error save\n",
    "log_train_error = np.zeros(10)\n",
    "log_test_error = np.zeros(10)\n",
    "\n",
    "opt_lambda = np.zeros(10)\n",
    "#coefficient_norm = np.zeros(len(lambda_interval))\n",
    "\n",
    "\n",
    "#* INIT FOR BASE-LINE\n",
    "base_line_train_err = np.zeros(10)\n",
    "base_line_test_err = np.zeros(10)\n",
    "\n",
    "#* INIT FOR ANN-CLASSIFICATION\n",
    "max_iter = 10000\n",
    "tolerance = 1e-6\n",
    "logging_frequency = 1000\n",
    "best_final_loss = 1e100\n",
    "n_replicates = 1\n",
    "ANN_error_test = np.zeros(K)\n",
    "ANN_error_train = np.zeros(K)\n",
    "\n",
    "h = [i for i in range(1,11)]\n",
    "\n",
    "C = len(np.unique(y))\n",
    "\n",
    "opt_h = np.zeros(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#* OUTER LOOP\n",
    "for m, (train_index, test_index) in tqdm(enumerate(CV_outer.split(X))):\n",
    "    X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "    # TODO: CHECK IF THIS STANDARDIZATION IS CORRECT.\n",
    "    mu = np.mean(X_train, axis=0)\n",
    "    sigma = np.std(X_train, axis = 0)\n",
    "\n",
    "    X_train = (X_train - mu)/sigma\n",
    "    X_test = (X_test - mu)/sigma\n",
    "\n",
    "    #* INNER LOOP\n",
    "    ## ## GOAL: finding best lambda & finding best number of hidden layers\n",
    "    CV_inner = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "    for j, (train_index_inner, test_index_inner) in enumerate(CV_inner.split(X_train, y_train)):\n",
    "        # printing\n",
    "        print(\"outer_loop: \" + str(m+1) + \"/10. inner_loop: \" + str(j+1) + \"/10.\",end='\\r')\n",
    "\n",
    "        X_train_inner, X_test_inner = X_train[train_index_inner, :], X_train[test_index_inner, :]\n",
    "        y_train_inner, y_test_inner = y_train[train_index_inner], y_train[test_index_inner]\n",
    "\n",
    "        #init for optimal h\n",
    "        h_error_train = np.zeros(K)\n",
    "        h_error_test = np.zeros(K)\n",
    "\n",
    "        ##########################\n",
    "        #* Finding the best lambda:\n",
    "        # allows us to iterate over a arbitray number of lambdas\n",
    "        for k in range(0, len(lambda_interval)):\n",
    "            #'newton-cg' gives better results\n",
    "            mdl = LogisticRegression(penalty='l2', solver='newton-cg', C=1/lambda_interval[k], max_iter=100)\n",
    "            # but the other is faster but outputs hella lot of warnings:\n",
    "            # mdl = LogisticRegression(penalty='l2', C=1/lambda_interval[k], max_iter=100)\n",
    "            \n",
    "            mdl.fit(X_train_inner, y_train_inner)\n",
    "\n",
    "            y_train_est_inner = mdl.predict(X_train_inner).T\n",
    "            y_test_est_inner = mdl.predict(X_test_inner).T\n",
    "            \n",
    "            train_error_rate[j,k] = np.sum(y_train_est_inner != y_train_inner) / len(y_train_inner)\n",
    "            test_error_rate[j,k] = np.sum(y_test_est_inner != y_test_inner) / len(y_test_inner)\n",
    "\n",
    "            # føler ikke det her skal bruges\n",
    "            #w_est = mdl.coef_[0] \n",
    "            #coefficient_norm[k] = np.sqrt(np.sum(w_est**2))\n",
    "\n",
    "\n",
    "        #* Finding the optimal amount of hidden layers:\n",
    "        X_train_inner, X_test_inner = torch.tensor(X_train_inner, dtype=torch.float), torch.tensor(X_test_inner, dtype=torch.float)\n",
    "        y_train_inner, y_test_inner = torch.tensor(y_train_inner), torch.tensor(y_test_inner)\n",
    "        # reshaping helps torch\n",
    "        #y_test_inner = torch.reshape(y_test_inner, (y_test_inner.shape[0],1))\n",
    "        #y_train_inner = torch.reshape(y_train_inner, (y_train_inner.shape[0],1))\n",
    "\n",
    "        n_hidden_units_inner = h[j]\n",
    "        model_inner = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units_inner), #M features to H hiden units\n",
    "                    # 1st transfer function, either Tanh or ReLU:\n",
    "                    #torch.nn.Tanh(),                            \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(n_hidden_units_inner, C),\n",
    "                    torch.nn.Softmax(dim=1)) #softmax as we want classes\n",
    "\n",
    "        # loss function for multinomial classification\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for r in range(n_replicates):\n",
    "            net_inner = model_inner()\n",
    "\n",
    "            torch.nn.init.xavier_uniform_(net_inner[0].weight)\n",
    "            torch.nn.init.xavier_uniform_(net_inner[2].weight)\n",
    "\n",
    "            optimizer = torch.optim.Adam(net_inner.parameters())\n",
    "\n",
    "            learning_curve = []\n",
    "            old_loss = 1e6\n",
    "            for i in range(max_iter):\n",
    "                y_est = net_inner(X_train_inner)\n",
    "                #y_class = torch.max(y_est, dim=1)[1]\n",
    "                loss = loss_fn(y_est, y_train_inner)\n",
    "                loss_value = loss.data.numpy()\n",
    "                learning_curve.append(loss_value)\n",
    "\n",
    "\n",
    "                p_delta_loss = np.abs(loss_value - old_loss)/old_loss\n",
    "                if p_delta_loss < tolerance: break\n",
    "                old_loss = loss_value\n",
    "\n",
    "\n",
    "                #if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "                #    print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "                #    print(print_str)\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "    \n",
    "\n",
    "            if loss_value < best_final_loss:\n",
    "                best_net = net_inner\n",
    "                best_final_loss = loss_value\n",
    "                best_learning_curve = learning_curve\n",
    "\n",
    "        y_pred_inner = net_inner(X_test_inner)\n",
    "        \n",
    "        softmax_logits = net_inner(torch.tensor(X_test, dtype=torch.float))\n",
    "        y_test_est = (torch.max(softmax_logits, dim=1)[1]).data.numpy() \n",
    "        h_error_test[j]  = np.sum(y_test_est != np.array(y_test))/len(y_test)\n",
    "\n",
    "\n",
    "         \n",
    "        #h_error_train[j] = best_final_loss\n",
    "\n",
    "    #* Optimal values found from inner loop\n",
    "    #  the smallesst of the means of the differnet lambdas\n",
    "    opt_lambda[m] = lambda_interval[np.argmin(np.mean(test_error_rate,axis=0))]\n",
    "    opt_h[m] = h[np.argmin(h_error_test)]\n",
    "\n",
    "\n",
    "    #* CLASSIFICATION\n",
    "    ##########################\n",
    "    ## BASELINE\n",
    "    # find out which class has the highest count\n",
    "    big_class = np.argmax([np.count_nonzero(y_train == i) for i in range(1,7)])\n",
    "    # assign everything to this class (calculate error rate)\n",
    "    base_line_train_err[m] = np.sum(y_train != big_class)/len(y_train)\n",
    "    base_line_test_err[m] = np.sum(y_test != big_class)/len(y_test)\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    ## Logistic Regression\n",
    "    log = LogisticRegression(penalty='l2', C=1/opt_lambda[m], max_iter=1000)\n",
    "    log.fit(X_train, y_train)\n",
    "    y_train_est = log.predict(X_train)\n",
    "    y_test_est = log.predict(X_test)\n",
    "\n",
    "    log_train_error[m] = np.sum(y_train_est != y_train)/len(y_train)\n",
    "    log_test_error[m] = np.sum(y_test_est != y_test)/len(y_test)\n",
    "\n",
    "    ##########################\n",
    "    ## ANN-Classification\n",
    "\n",
    "    model = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, int(opt_h[m])), #M features to H hiden units\n",
    "                    # 1st transfer function, either Tanh or ReLU:\n",
    "                    #torch.nn.Tanh(),                            \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(int(opt_h[m]), C), # H hidden units to 1 output neuron\n",
    "                    torch.nn.Softmax(dim=1) # final tranfer function\n",
    "                    )\n",
    "\n",
    "\n",
    "    X_train, y_train = torch.tensor(X_train, dtype = torch.float), torch.tensor(y_train)\n",
    "    X_test, y_test = torch.tensor(X_test, dtype = torch.float), torch.tensor(y_test)\n",
    "    n_replicates = 3\n",
    "    for r in range(n_replicates):\n",
    "        net = model()\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(net[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(net[2].weight)\n",
    "\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "        learning_curve = []\n",
    "        old_loss = 1e6\n",
    "        for i in range(max_iter):\n",
    "            y_est = net(X_train)\n",
    "            #y_class = torch.max(y_est, dim=1)[1]\n",
    "            loss = loss_fn(y_est, y_train)\n",
    "            loss_value = loss.data.numpy()\n",
    "            learning_curve.append(loss_value)\n",
    "\n",
    "\n",
    "            p_delta_loss = np.abs(loss_value - old_loss)/old_loss\n",
    "            if p_delta_loss < tolerance: break\n",
    "            old_loss = loss_value\n",
    "\n",
    "\n",
    "            if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "                #print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "                ###print(print_str)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        #print('\\t\\tFinal Loss:')\n",
    "        #print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "        ##print(print_str)\n",
    "\n",
    "        if loss_value < best_final_loss:\n",
    "            best_net = net\n",
    "            best_final_loss = loss_value\n",
    "            best_learning_curve = learning_curve\n",
    "        \n",
    "    \n",
    "    softmax_logits = net(torch.tensor(X_test, dtype=torch.float))\n",
    "    y_test_est = (torch.max(softmax_logits, dim=1)[1]).data.numpy() \n",
    "    ANN_error_test[m] = np.sum(y_test_est != np.array(y_test))/len(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# WARNING: Inden du kligger på den flotte trekant.\n",
    "# Det tager cirka 20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold i    | h               E_ANN           lambda          E_log           E_base         \n",
      "-----------------------------------------------------------------------------------------\n",
      "1               | 1               0.364           0.000000        0.227           0.6363636      \n",
      "2               | 1               0.318           0.086851        0.273           0.6818182      \n",
      "3               | 1               0.409           0.138950        0.409           0.5909091      \n",
      "4               | 1               0.364           0.005179        0.318           0.5000000      \n",
      "5               | 1               0.667           1.456348        0.381           0.7619048      \n",
      "6               | 1               0.571           0.910298        0.524           0.6666667      \n",
      "7               | 1               0.476           0.568987        0.286           0.6666667      \n",
      "8               | 1               0.429           0.054287        0.571           0.6190476      \n",
      "9               | 1               0.524           0.222300        0.333           0.8095238      \n",
      "10              | 1               0.429           0.568987        0.429           0.8095238      \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15} | {:<15} {:<15} {:<15} {:<15} {:<15}\".format('Outer fold i', 'h', 'E_ANN', 'lambda', 'E_log', 'E_base'))\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(\"{:<15} | {:<15} {:<15.3f} {:<15.6f} {:<15.3f} {:<15.7f}\".format(i+1, int(opt_h[i]), ANN_error_test[i], opt_lambda[i], log_test_error[i], base_line_test_err[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('AndSem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d62badb4c3b216db4db7804bb7696037312a1bbdb2c7465cfb03cc785a825525"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
